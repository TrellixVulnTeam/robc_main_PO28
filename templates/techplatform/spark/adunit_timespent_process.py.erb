import mysql.connector, base64, sys 
from mysql.connector.errors import Error
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext
from datetime import timedelta, date
import subprocess, uuid, os

class QueryAnalytics():
  
  def __init__(self, user, password, hostname, database):
    self.connection_string = {
        "user"     : user,
        "password" : password,
        "host": hostname,
        "database": database
    } 

  def execute_query(self, query):
    cnx = mysql.connector.connect(**self.connection_string)
    cur = cnx.cursor()
    for result in cur.execute(query, multi = True):
      pass
    cnx.commit()
    cur.close()
    cnx.close()

def daterange(start_date, end_date):
  for n in range(int (((end_date - start_date).days) + 1)):
    yield start_date + timedelta(n)

def fileExists(_path):
  _result = False
  
  p = subprocess.Popen(['hdfs', 'dfs', '-find', _path], stdout=subprocess.PIPE)

  for _stdout in p.stdout:
    if (_stdout.find('No such file or directory', 0) < 0):
      _result = True

  return _result


def main(argv):

  environment = '<%= domain.split('.')[1] %>'
  conf = SparkConf().setAppName("Ad Unit Time Spent Data {0}".format(environment.upper()))
  conf.set("spark.app.id", uuid.uuid4())
  sc = SparkContext(conf=conf)
  sqlContext = SQLContext(sc)

  hdfs = 'zootpprd'
  start_date = date.today() - timedelta(days=1)
  end_date = date.today()

  user = 'spark'
  database = 'analytics'
  cnn_string = {}
  cnn_string['dev'] = {'server':'sql1v-bd.og.dev.lax.gnmedia.net', 'password':'QnJxU3A3YXduT0Uw'}
  cnn_string['stg'] = {'server':'sql1v-bd.og.stg.lax.gnmedia.net', 'password':'bHFTOG5QOVBqb0ZN'}
  #cnn_string['prd'] = {'server':'VIP-SQLRW-BD.OG.PRD.LAX.GNMEDIA.NET', 'password':'YlR0cEQwaUdzUU85'}
  cnn_string['prd'] = {'server':'sql1v-bd.og.prd.lax.gnmedia.net', 'password':'YlR0cEQwaUdzUU85'}
  
  qa = QueryAnalytics(user, base64.b64decode(cnn_string[environment]['password']), cnn_string[environment]['server'], database)
  mysql_url="jdbc:mysql://{0}:3306/analytics?user={1}&password={2}".format(cnn_string[environment]['server'], user, base64.b64decode(cnn_string[environment]['password']))

  query = "TRUNCATE TABLE timespent_unit_tmp"
  qa.execute_query(query)

  for single_date in daterange(start_date, end_date):
    file = "hdfs://{0}/spark/origin/processed/{1}/{2}/{3}/beacon_data.{4}".format(hdfs, environment, single_date.year, single_date.strftime("%m"), single_date.strftime("%d"))
    print "Processing file {0} ....".format(file)
    if (fileExists(file)):
      try:
        select_origin = sqlContext.read.json("hdfs://{0}/spark/origin/processed/{1}/{2}/{3}/beacon_data.{4}".format(hdfs, environment, single_date.year, single_date.strftime("%m"), single_date.strftime("%d")))
      except Exception as ex:
        print str(ex)
    
      select_origin.registerTempTable("pixel_data")
      analytics = sqlContext.sql("SELECT IF(adId IS NULL, 0, adId) as adid, " \
                                 "       ts as created_at, " \
                                 "       IF(timespent IS NULL, 0, timespent) as timespent, " \
                                 "       IF(sessionid IS NULL, '', sessionid) as sessionid " \
                                 "FROM pixel_data " \
                                 "WHERE version >= 2 AND event = 'Time Spent'")
      analytics.write.jdbc(url=mysql_url, table="analytics.timespent_unit_tmp", mode="append")

  query = "CALL ea_og_sp_insert_adunit_timespent_data()"
  qa.execute_query(query)
  sc.stop()

if __name__ == "__main__":
   main(sys.argv[1:])
